{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import chakin\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gzip\n",
    "from gensim import models as gmodel\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset\n",
    "annotation_zip = kr.utils.get_file('/content/drive/My Drive/colab/coco_dataset/captions.zip',\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = False)\n",
    "\n",
    "name_of_zip = 'train2014.zip'\n",
    "image_zip = tf.keras.utils.get_file(name_of_zip,\n",
    "                              cache_subdir=os.path.abspath('.'),\n",
    "                                    origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                    extract = True)\n",
    "  \n",
    "name_of_zip = 'test2014.zip'\n",
    "image_zip = kr.utils.get_file(name_of_zip,\n",
    "                                    origin = 'http://images.cocodataset.org/zips/test2014.zip',\n",
    "                                    extract = False)\n",
    "\n",
    "name_of_zip = 'val2014.zip'\n",
    "image_zip = tf.keras.utils.get_file(name_of_zip,\n",
    "                              cache_subdir=os.path.abspath('.'),\n",
    "                                    origin = 'http://images.cocodataset.org/zips/val2014.zip',\n",
    "                                    extract = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation_file = '/content/drive/My Drive/colab/coco_dataset/captions/annotations/captions_train2014.json'\n",
    "test_annotation_file = '/content/drive/My Drive/colab/coco_dataset/captions/annotations/captions_test2014.json'\n",
    "val_annotation_file = '/content/drive/My Drive/colab/coco_dataset/captions/annotations/captions_val2014.json'\n",
    "train_incep_path = '/content/drive/My Drive/colab/coco_dataset/train_incep/'\n",
    "test_incep_path = '/content/drive/My Drive/colab/coco_dataset/test_incep/'\n",
    "val_incep_path = '/content/drive/My Drive/colab/coco_dataset/val_incep/'\n",
    "train_img_path = '/content/drive/My Drive/colab/coco_dataset/train_img/'\n",
    "test_img_path = '/content/drive/My Drive/colab/coco_dataset/test_img/'\n",
    "val_img_path = '/content/drive/My Drive/colab/coco_dataset/val_img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = glob('/content/drive/My Drive/colab/coco_dataset/train_img/*/*')\n",
    "val_paths = glob('/content/drive/My Drive/colab/coco_dataset/val_img/*/*')\n",
    "\n",
    "print(len(train_paths))\n",
    "print(len(val_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move unzipped files in subdirectories\n",
    "os.mkdir(train_img_path)\n",
    "os.mkdir(val_img_path)\n",
    "for i in range(10):\n",
    "    os.mkdir(train_img_path+str(i)+'/')\n",
    "    os.mkdir(val_img_path+str(i)+'/')\n",
    "\n",
    "def mv1(arg):\n",
    "    i,f = arg\n",
    "    i = i%10\n",
    "    dest = train_img_path + str(i) + '/' + f[f.rfind('/')+1:]\n",
    "    shutil.move(f,dest)\n",
    "\n",
    "def mv2(arg):\n",
    "    i,f = arg\n",
    "    i = i%10\n",
    "    dest = val_img_path + str(i) + '/' + f[f.rfind('/')+1:]\n",
    "    shutil.move(f,dest)\n",
    "    \n",
    "lst = list(map(mv1,enumerate(train_paths)))\n",
    "lst = list(map(mv2,enumerate(val_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inception model for feature extraction\n",
    "incep_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = incep_model.input\n",
    "hidden_layer = incep_model.layers[-1].output\n",
    "\n",
    "incep_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "print(incep_features_extract_model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "encode_train = train_paths\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = incep_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        f = p.numpy().decode(\"utf-8\")\n",
    "        pof = train_incep_path + f[f.rfind('train_img')+10:f.rfind('.')]\n",
    "        if not os.path.exists(pof):\n",
    "            np.save(pof, bf.numpy())\n",
    "            \n",
    "encode_val = val_paths\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_val)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = incep_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        f = p.numpy().decode(\"utf-8\")\n",
    "        pof = val_incep_path + f[f.rfind('val_img')+8:f.rfind('.')]\n",
    "        if not os.path.exists(pof):\n",
    "            np.save(pof, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_paths = glob(train_img_path+'*/*')\n",
    "val_img_paths = glob(val_img_path+'*/*')\n",
    "\n",
    "print(len(train_img_paths))\n",
    "print(len(val_img_paths))\n",
    "\n",
    "train_incep_paths = glob(train_incep_path+'*/*npy')\n",
    "val_incep_paths = glob(val_incep_path+'*/*npy')\n",
    "\n",
    "print(len(train_incep_paths))\n",
    "print(len(val_incep_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing tensor shapes after each layer of model.\n",
    "features = tf.random.uniform((32,64,2048)) #example input tensor\n",
    "print('features:',features.shape)\n",
    "print()\n",
    "\n",
    "print('encoder-')\n",
    "features = tf.keras.layers.BatchNormalization()(features)\n",
    "feat_mean = tf.reduce_mean(features, 1)\n",
    "print('feat_mean:',feat_mean.shape)\n",
    "h_state = tf.keras.layers.Dense(600)(feat_mean)\n",
    "h_state = tf.nn.tanh(h_state)\n",
    "print('h_state:',h_state.shape)\n",
    "c_state = tf.keras.layers.Dense(600)(feat_mean)\n",
    "c_state = tf.nn.tanh(c_state)\n",
    "print('c_state:',c_state.shape)\n",
    "fc = tf.keras.layers.Dense(300)\n",
    "features = fc(features)\n",
    "features = tf.nn.relu(features)\n",
    "print('features:',features.shape)\n",
    "print()\n",
    "\n",
    "print('attention-')\n",
    "W1 = tf.keras.layers.Dense(600)\n",
    "W2 = tf.keras.layers.Dense(600)\n",
    "V = tf.keras.layers.Dense(1)\n",
    "hidden = tf.expand_dims(h_state, 1)\n",
    "print('hidden:',hidden.shape)\n",
    "W_features = W1(features)\n",
    "W_hidden = W2(hidden)\n",
    "print('W_features:',W_features.shape)\n",
    "print('W_hidden:',W_hidden.shape)\n",
    "W_add = W_features+W_hidden\n",
    "print('W_add:',W_add.shape)\n",
    "score = tf.nn.tanh(W_add)\n",
    "print('score:',score.shape)\n",
    "V_score = V(score)\n",
    "print('V_score:',V_score.shape)\n",
    "attn_weights = tf.nn.softmax(V_score)\n",
    "print('attn_weights:',attn_weights.shape)\n",
    "context_vector = attn_weights * features\n",
    "print('context_vector:',context_vector.shape)\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "print('context_vector:',context_vector.shape)\n",
    "print()\n",
    "\n",
    "print('decoder-')\n",
    "x  = tf.convert_to_tensor([tokenizer.word_index['<start>']] * 32, 1)\n",
    "print('x:',x.shape)\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[embed])(x)\n",
    "print('x:',x.shape)\n",
    "beta = tf.keras.layers.Dense(1)(h_state)\n",
    "print('beta:',beta.shape)\n",
    "beta = tf.nn.sigmoid(beta)\n",
    "print('beta:',beta.shape)\n",
    "context_vector = tf.multiply(beta, context_vector, name='selected_context')\n",
    "print('context_vector:',context_vector.shape)\n",
    "x_in = tf.concat([x,context_vector], axis=1)\n",
    "print('x_in:',x_in.shape)\n",
    "output,[c_state,h_state] = tf.keras.layers.LSTMCell(units=600,recurrent_initializer='glorot_uniform')(x,states=[c_state,h_state])\n",
    "print('output:',output.shape)\n",
    "print('c_state:',c_state.shape)\n",
    "print('h_state:',h_state.shape)\n",
    "h_state = tf.nn.dropout(h_state,0.5)\n",
    "W_h = tf.keras.layers.Dense(300)(h_state)\n",
    "print('W_h:',W_h.shape)\n",
    "W_context = tf.keras.layers.Dense(300,use_bias=False)(context_vector)\n",
    "print('W_context:',W_context.shape)\n",
    "h_logits = W_h + W_context + x\n",
    "print('h_logits:',h_logits.shape)\n",
    "h_logits = tf.nn.tanh(h_logits)\n",
    "h_logits = tf.nn.dropout(h_logits,0.5)\n",
    "print('h_logits:',h_logits.shape)\n",
    "out_logits = tf.keras.layers.Dense(vocab_size)(h_logits)\n",
    "print('out_logits:',out_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [attn_weights]*52\n",
    "alpha_c = 0.01\n",
    "alphas = tf.stack(alpha_list)\n",
    "print('alphas:',alphas.shape)\n",
    "alphas = tf.transpose(alphas, (1, 0, 2,3))\n",
    "print('alphas:',alphas.shape)\n",
    "alphas_all = tf.reduce_sum(alphas, 1)\n",
    "print('alphas_all:',alphas_all.shape)\n",
    "alpha_reg = alpha_c * tf.reduce_sum((52./64 - alphas_all) ** 2,1)\n",
    "print('alpha_reg:',alpha_reg.shape)\n",
    "alpha_reg = tf.reduce_mean(alpha_reg)\n",
    "print('alpha_reg:',alpha_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
